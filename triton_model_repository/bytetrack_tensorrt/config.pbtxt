name: "bytetrack_tensorrt"
platform: "tensorrt_plan"
max_batch_size: 1

input [
  {
    name: "images"
    data_type: TYPE_FP16
    dims: [ 3, 640, 640 ]
  }
]

output [
  {
    name: "output"
    data_type: TYPE_FP16
    dims: [ 8400, 6 ]
  }
]

# Dynamic batching configuration
# Note: TensorRT engine supports max_batch_size=1, so we can only batch single requests
# For true multi-batch support, need to rebuild TensorRT engine with dynamic shapes
dynamic_batching {
  # Only batch size 1 is supported by current TensorRT engine
  preferred_batch_size: [ 1 ]

  # Maximum queue delay in microseconds (500us = 0.5ms)
  # Short delay to reduce latency while still allowing request batching
  max_queue_delay_microseconds: 500

  # Preserve ordering of requests for consistent results
  preserve_ordering: true

  # Queue policy - reject requests if queue is full
  default_queue_policy {
    timeout_action: REJECT
    default_timeout_microseconds: 10000
    allow_timeout_override: true
    max_queue_size: 32
  }
}

# Instance group configuration
instance_group [
  {
    # Number of instances per GPU
    count: 1
    
    # GPU device
    kind: KIND_GPU
    
    # Use GPU 0
    gpus: [ 0 ]
  }
]

# Optimization settings
optimization {
  # Enable CUDA graphs for faster inference
  cuda {
    graphs: true
    graph_spec {
      batch_size: 1
      input {
        key: "images"
        value {
          dim: [ 3, 640, 640 ]
        }
      }
    }
  }
}

# Model warmup
model_warmup [
  {
    name: "batch_size_1"
    batch_size: 1
    inputs {
      key: "images"
      value {
        data_type: TYPE_FP16
        dims: [ 3, 640, 640 ]
        zero_data: true
      }
    }
  }
]

