name: "bytetrack_tensorrt"
platform: "tensorrt_plan"
max_batch_size: 1

input [
  {
    name: "images"
    data_type: TYPE_FP16
    dims: [ 3, 640, 640 ]
  }
]

output [
  {
    name: "output"
    data_type: TYPE_FP16
    dims: [ 8400, 6 ]
  }
]

# Dynamic batching configuration
# Note: TensorRT engine supports max_batch_size=1, so we can only batch single requests
# For true multi-batch support, need to rebuild TensorRT engine with dynamic shapes
dynamic_batching {
  # Only batch size 1 is supported by current TensorRT engine
  preferred_batch_size: [ 1 ]

  # Maximum queue delay in microseconds (5000us = 5ms)
  # Optimized for multi-stream throughput (4-8 cameras)
  # Lower value = lower latency, higher value = better throughput
  max_queue_delay_microseconds: 5000

  # Preserve ordering disabled for better throughput
  preserve_ordering: false

  # Queue policy - optimized for multi-stream processing
  default_queue_policy {
    timeout_action: REJECT
    default_timeout_microseconds: 20000  # 20ms timeout
    allow_timeout_override: true
    max_queue_size: 256  # Large queue for multi-stream
  }
}

# Instance group configuration
# Multiple instances enable parallel processing of concurrent requests
# Each instance uses ~500MB GPU memory
# 16 instances = 16 parallel requests = ~8GB GPU memory (optimized for 12-16+ streams)
instance_group [
  {
    # Number of instances per GPU (high throughput configuration)
    # Recommended: 4 (4-8 cams), 8 (8-12 cams), 16 (12-16+ cams)
    count: 16

    # GPU device
    kind: KIND_GPU

    # Use GPU 0
    gpus: [ 0 ]
  }
]

# Optimization settings
optimization {
  # Enable CUDA graphs for faster inference
  cuda {
    graphs: true
    graph_spec {
      batch_size: 1
      input {
        key: "images"
        value {
          dim: [ 3, 640, 640 ]
        }
      }
    }
  }
}

# Model warmup
model_warmup [
  {
    name: "batch_size_1"
    batch_size: 1
    inputs {
      key: "images"
      value {
        data_type: TYPE_FP16
        dims: [ 3, 640, 640 ]
        zero_data: true
      }
    }
  }
]

