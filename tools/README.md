# TensorRT Optimization Tools

C√¥ng c·ª• ƒë·ªÉ export v√† t·ªëi ∆∞u YOLOX model v·ªõi TensorRT cho t·ªëc ƒë·ªô inference nhanh h∆°n 3-5x.

## üìã Y√™u c·∫ßu

### Dependencies c∆° b·∫£n
```bash
pip install onnx onnxruntime-gpu onnx-simplifier
```

### TensorRT (cho conversion v√† inference)
```bash
# Option 1: C√†i t·ª´ NVIDIA
# Download t·ª´: https://developer.nvidia.com/tensorrt
# L√†m theo h∆∞·ªõng d·∫´n c√†i ƒë·∫∑t

# Option 2: D√πng pip (n·∫øu c√≥ s·∫µn)
pip install tensorrt pycuda
```

## üöÄ Workflow

### B∆∞·ªõc 1: Export ONNX t·ª´ PyTorch

Export model PyTorch sang ONNX v·ªõi FP32 precision:

```bash
python tools/export_onnx.py \
    --model models/bytetrack_x_mot17.pth.tar \
    --output models/bytetrack_x_mot17_fp32.onnx \
    --size 640 640 \
    --opset 11
```

**Tham s·ªë:**
- `--model`: Path ƒë·∫øn PyTorch weights (.pth.tar)
- `--output`: Path output ONNX model
- `--size`: Input size (height width), m·∫∑c ƒë·ªãnh: 640 640
- `--opset`: ONNX opset version (11, 12, ho·∫∑c 13), khuy·∫øn ngh·ªã: 11
- `--dynamic-batch`: Enable dynamic batch size (kh√¥ng khuy·∫øn ngh·ªã cho TensorRT)
- `--no-simplify`: Skip ONNX simplification

**Best Practices:**
- ‚úÖ Export FP32 (TensorRT s·∫Ω t·ª± optimize)
- ‚úÖ Fixed batch size = 1 (nhanh nh·∫•t)
- ‚úÖ Opset 11 ho·∫∑c 12 (t∆∞∆°ng th√≠ch t·ªët v·ªõi TensorRT)
- ‚ùå KH√îNG d√πng `fuse_model` tr∆∞·ªõc khi export
- ‚ùå KH√îNG d√πng `.half()` tr∆∞·ªõc khi export

### B∆∞·ªõc 2: Verify ONNX Model

Ki·ªÉm tra ONNX model structure v√† inference:

```bash
python tools/verify_onnx.py \
    --model models/bytetrack_x_mot17_fp32.onnx \
    --test-image data/test_image.jpg  # Optional
```

**Output:**
- Model information (IR version, opset, input/output shapes)
- ONNX validity check
- Inference test v·ªõi onnxruntime
- Benchmark (100 iterations)
- Accuracy comparison v·ªõi PyTorch (n·∫øu c√≥ test image)

### B∆∞·ªõc 3: Convert ONNX sang TensorRT

Convert ONNX sang TensorRT engine v·ªõi FP16 precision:

```bash
python tools/convert_tensorrt.py \
    --onnx models/bytetrack_x_mot17_fp32.onnx \
    --output models/bytetrack_x_mot17_fp16.trt \
    --fp16 \
    --workspace 2048
```

**Tham s·ªë:**
- `--onnx`: Path ƒë·∫øn ONNX model
- `--output`: Path output TensorRT engine (auto-generated n·∫øu kh√¥ng ch·ªâ ƒë·ªãnh)
- `--fp16`: Enable FP16 precision (khuy·∫øn ngh·ªã, ~3-4x speedup)
- `--int8`: Enable INT8 precision (c·∫ßn calibration, ~4-5x speedup)
- `--workspace`: Max workspace size in MB (m·∫∑c ƒë·ªãnh: 2048)
- `--verbose`: Verbose logging

**Precision Options:**
- **FP32**: Baseline, kh√¥ng t·ªëi ∆∞u
- **FP16**: ~3-4x nhanh h∆°n, 99.5% accuracy ‚úÖ Khuy·∫øn ngh·ªã
- **INT8**: ~4-5x nhanh h∆°n, 98-99% accuracy (c·∫ßn calibration)

### B∆∞·ªõc 4: Benchmark PyTorch vs TensorRT

So s√°nh t·ªëc ƒë·ªô v√† accuracy:

```bash
python tools/benchmark.py \
    --pytorch-model models/bytetrack_x_mot17.pth.tar \
    --tensorrt-engine models/bytetrack_x_mot17_fp16.trt \
    --video data/test_video.mp4 \
    --num-frames 100 \
    --iterations 100
```

**Tham s·ªë:**
- `--pytorch-model`: PyTorch model path
- `--tensorrt-engine`: TensorRT engine path
- `--video`: Test video path
- `--num-frames`: S·ªë frames ƒë·ªÉ test (m·∫∑c ƒë·ªãnh: 100)
- `--warmup`: Warmup iterations (m·∫∑c ƒë·ªãnh: 10)
- `--iterations`: Benchmark iterations (m·∫∑c ƒë·ªãnh: 100)
- `--skip-pytorch`: Skip PyTorch benchmark
- `--skip-tensorrt`: Skip TensorRT benchmark

**Output:**
- Timing statistics (avg, std, min, max, P50, P95, P99)
- FPS comparison
- Speedup ratio
- Accuracy metrics (precision, recall, F1)

## üéØ S·ª≠ d·ª•ng trong Production

### C·∫•u h√¨nh Backend

Ch·ªânh s·ª≠a `configs/config.yaml`:

```yaml
detection:
  # Ch·ªçn backend: pytorch ho·∫∑c tensorrt
  backend: tensorrt  # ƒê·ªïi t·ª´ pytorch sang tensorrt
  
  # PyTorch model paths
  model_path_mot17: models/bytetrack_x_mot17.pth.tar
  
  # TensorRT engine paths
  tensorrt_engine_mot17: models/bytetrack_x_mot17_fp16.trt
  
  # Detection parameters
  conf_threshold: 0.5
  nms_threshold: 0.45
  test_size: [640, 640]
```

### Ch·∫°y Pipeline v·ªõi TensorRT

```python
from core.preloaded_manager import PreloadedPipelineManager

# Initialize v·ªõi TensorRT backend
manager = PreloadedPipelineManager()
manager.initialize()  # S·∫Ω load TensorRT detector theo config

# Detector s·∫Ω t·ª± ƒë·ªông d√πng TensorRT
detector = manager.detector
detections = detector.detect(frame)
```

## üìä Expected Performance

### RTX 3090 (v√≠ d·ª•)

| Backend | Precision | Avg Time | FPS | Speedup | Accuracy |
|---------|-----------|----------|-----|---------|----------|
| PyTorch | FP16 | ~12ms | ~83 | 1.0x | 100% |
| TensorRT | FP16 | ~3ms | ~333 | 4.0x | 99.5% |
| TensorRT | INT8 | ~2ms | ~500 | 6.0x | 98-99% |

*L∆∞u √Ω: K·∫øt qu·∫£ th·ª±c t·∫ø ph·ª• thu·ªôc v√†o GPU, CUDA version, TensorRT version*

## üîß Troubleshooting

### L·ªói: ONNX export failed

**Nguy√™n nh√¢n:** Model c√≥ operations kh√¥ng support b·ªüi ONNX

**Gi·∫£i ph√°p:**
- Th·ª≠ opset version kh√°c (11, 12, 13)
- Ki·ªÉm tra PyTorch version compatibility

### L·ªói: TensorRT build failed

**Nguy√™n nh√¢n:** ONNX model c√≥ operations kh√¥ng support b·ªüi TensorRT

**Gi·∫£i ph√°p:**
- Verify ONNX model tr∆∞·ªõc: `python tools/verify_onnx.py`
- Th·ª≠ workspace size l·ªõn h∆°n: `--workspace 4096`
- Ki·ªÉm tra TensorRT version compatibility

### L·ªói: CUDNN_STATUS_BAD_PARAM

**Nguy√™n nh√¢n:** CUDNN version kh√¥ng t∆∞∆°ng th√≠ch

**Gi·∫£i ph√°p:**
- C√†i ƒë√∫ng CUDNN version cho TensorRT
- Ho·∫∑c d√πng TensorRT ƒë·ªÉ inference thay v√¨ onnxruntime

### L·ªói: Engine file not found

**Nguy√™n nh√¢n:** Ch∆∞a convert ONNX sang TensorRT

**Gi·∫£i ph√°p:**
```bash
python tools/convert_tensorrt.py \
    --onnx models/bytetrack_x_mot17_fp32.onnx
```

## üìù Notes

1. **ONNX Export:**
   - Lu√¥n export FP32, ƒë·ªÉ TensorRT t·ª± optimize
   - Kh√¥ng d√πng `fuse_model()` ho·∫∑c `.half()` tr∆∞·ªõc khi export
   - Fixed batch size = 1 cho t·ªëc ƒë·ªô t·ªët nh·∫•t

2. **TensorRT Conversion:**
   - FP16 l√† l·ª±a ch·ªçn t·ªët nh·∫•t (speedup cao, accuracy t·ªët)
   - INT8 c·∫ßn calibration dataset ƒë·ªÉ ƒë·∫°t accuracy t·ªët
   - Engine file ph·ª• thu·ªôc v√†o GPU, kh√¥ng portable

3. **Production:**
   - Build engine tr√™n GPU production
   - Ki·ªÉm tra accuracy tr∆∞·ªõc khi deploy
   - Monitor performance v√† accuracy trong production

## üîó References

- [TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/)
- [ONNX Documentation](https://onnx.ai/)
- [YOLOX GitHub](https://github.com/Megvii-BaseDetection/YOLOX)

