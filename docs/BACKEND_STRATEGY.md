# Detection Backend Strategy Guide

## üìã Overview

Person ReID System h·ªó tr·ª£ 3 detection backends v·ªõi c√°c ƒë·∫∑c ƒëi·ªÉm v√† use cases kh√°c nhau:

1. **PyTorch** - Standard inference v·ªõi PyTorch
2. **TensorRT** - Optimized GPU inference v·ªõi NVIDIA TensorRT
3. **Triton Inference Server** - Multi-stream optimization v·ªõi dynamic batching

Document n√†y gi√∫p b·∫°n l·ª±a ch·ªçn backend ph√π h·ª£p cho use case c·ªßa m√¨nh.

---

## üéØ Backend Comparison

### Performance Comparison

| Backend | Single Stream | Multi-Stream (4 cams) | GPU Memory | Setup Complexity |
|---------|---------------|----------------------|------------|------------------|
| **PyTorch** | 21-22 FPS | 21-22 FPS total | ~2GB | ‚≠ê Easy |
| **TensorRT** | 28-30 FPS | 28-30 FPS total | ~2GB | ‚≠ê‚≠ê Medium |
| **Triton** | 26-28 FPS | **66+ FPS total** | ~3-4GB | ‚≠ê‚≠ê‚≠ê Advanced |

### Latency Comparison

| Backend | Avg Inference Time | P95 Latency | P99 Latency |
|---------|-------------------|-------------|-------------|
| **PyTorch FP16** | 45.63ms | ~50ms | ~55ms |
| **TensorRT FP16** | 35.58ms | ~40ms | ~45ms |
| **Triton + TensorRT** | **22.89ms** | **~25ms** | **~30ms** |

**Key Insight**: Triton gi·∫£m latency **50%** so v·ªõi PyTorch v√† **36%** so v·ªõi TensorRT standalone.

---

## üîç Detailed Backend Analysis

### 1. PyTorch Backend

**Architecture:**
```
Frame ‚Üí Preprocess ‚Üí PyTorch Model (GPU) ‚Üí Postprocess ‚Üí Detections
```

**Pros:**
- ‚úÖ D·ªÖ setup, kh√¥ng c·∫ßn convert model
- ‚úÖ Debugging d·ªÖ d√†ng v·ªõi Python
- ‚úÖ Flexible, d·ªÖ modify model
- ‚úÖ H·ªó tr·ª£ c·∫£ CPU v√† GPU

**Cons:**
- ‚ùå Ch·∫≠m nh·∫•t trong 3 backends
- ‚ùå Kh√¥ng t·ªëi ∆∞u cho production
- ‚ùå Kh√¥ng h·ªó tr·ª£ dynamic batching
- ‚ùå Python GIL overhead

**Use Cases:**
- Development v√† debugging
- Single camera v·ªõi FPS th·∫•p (< 15 FPS)
- Prototype v√† testing
- Kh√¥ng c√≥ GPU ho·∫∑c GPU y·∫øu

**Configuration:**
```yaml
# configs/config.yaml
detection:
  backend: pytorch
  model_path: models/bytetrack_x_mot17.pth.tar
  conf_threshold: 0.01
  nms_threshold: 0.7
  test_size: [640, 640]
  fp16: true  # Enable FP16 for faster inference
```

**Code Example:**
```python
from core import YOLOXDetector

detector = YOLOXDetector(
    model_path="models/bytetrack_x_mot17.pth.tar",
    model_type="mot17",
    conf_thresh=0.01,
    nms_thresh=0.7,
    test_size=(640, 640),
    fp16=True
)

detections = detector.detect(frame)  # Returns (N, 7) array
```

---

### 2. TensorRT Backend

**Architecture:**
```
Frame ‚Üí Preprocess ‚Üí TensorRT Engine (GPU) ‚Üí Postprocess ‚Üí Detections
```

**Pros:**
- ‚úÖ **1.3-1.5x faster** than PyTorch
- ‚úÖ Optimized CUDA kernels
- ‚úÖ FP16 precision v·ªõi minimal accuracy loss
- ‚úÖ Low latency (~35ms)
- ‚úÖ Kh√¥ng c·∫ßn Docker

**Cons:**
- ‚ùå C·∫ßn convert ONNX ‚Üí TensorRT
- ‚ùå Engine specific to GPU architecture
- ‚ùå Kh√¥ng h·ªó tr·ª£ dynamic batching
- ‚ùå C·∫ßn rebuild engine khi ƒë·ªïi GPU

**Use Cases:**
- Single camera v·ªõi high FPS (25-30 FPS)
- 2-3 cameras sequential processing
- Production deployment tr√™n single GPU
- Khi c·∫ßn low latency nh∆∞ng kh√¥ng c·∫ßn Triton complexity

**Setup Steps:**

1. **Export ONNX:**
```bash
python tools/export_onnx.py \
    --model models/bytetrack_x_mot17.pth.tar \
    --output models/bytetrack_x_mot17_fp16.onnx \
    --opset 11
```

2. **Convert to TensorRT:**
```bash
python tools/convert_tensorrt.py \
    --onnx models/bytetrack_x_mot17_fp16.onnx \
    --output models/bytetrack_x_mot17_fp16.trt \
    --fp16 \
    --workspace 4096
```

3. **Configure:**
```yaml
# configs/config.yaml
detection:
  backend: tensorrt
  tensorrt:
    engine_path: models/bytetrack_x_mot17_fp16.trt
    fp16: true
```

4. **Code Example:**
```python
from core import TensorRTDetector

detector = TensorRTDetector(
    engine_path="models/bytetrack_x_mot17_fp16.trt",
    conf_thresh=0.01,
    nms_thresh=0.7,
    test_size=(640, 640)
)

detections = detector.detect(frame)  # Returns (N, 7) array
```

---

### 3. Triton Inference Server Backend

**Architecture:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Camera 1‚îÇ  ‚îÇ Camera 2‚îÇ  ‚îÇ Camera N‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ            ‚îÇ            ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ gRPC (concurrent)
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ Triton Server  ‚îÇ
          ‚îÇ Dynamic Batch  ‚îÇ ‚Üê Queues requests
          ‚îÇ (max 500Œºs)    ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ TensorRT Engine‚îÇ
          ‚îÇ CUDA Graphs    ‚îÇ ‚Üê Optimized execution
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Pros:**
- ‚úÖ **2-3x faster** than PyTorch for multi-stream
- ‚úÖ **Dynamic batching** - t·ª± ƒë·ªông g·ªôp requests
- ‚úÖ **Concurrent execution** - x·ª≠ l√Ω ƒë·ªìng th·ªùi nhi·ªÅu streams
- ‚úÖ **CUDA Graphs** - gi·∫£m kernel launch overhead
- ‚úÖ **Model versioning** - hot-reload models
- ‚úÖ **Metrics & monitoring** - Prometheus metrics
- ‚úÖ **Production-ready** - battle-tested by NVIDIA

**Cons:**
- ‚ùå Complex setup (Docker, model repository)
- ‚ùå C·∫ßn hi·ªÉu Triton configuration
- ‚ùå Overhead cho single stream
- ‚ùå C·∫ßn th√™m ~1-2GB GPU memory

**Use Cases:**
- **4+ cameras** concurrent processing
- High throughput requirements (> 60 FPS total)
- Production deployment v·ªõi multiple streams
- Khi c·∫ßn monitoring v√† metrics
- Microservices architecture

**Setup Steps:**

See [TRITON_DEPLOYMENT.md](../deployment/TRITON_DEPLOYMENT.md) for detailed setup.

**Quick Start:**
```bash
# 1. Setup model repository
cd deployment
bash setup_triton.sh

# 2. Start Triton server
sudo docker compose up -d triton

# 3. Verify
curl http://localhost:8100/v2/health/ready

# 4. Configure
# Edit configs/config.yaml:
detection:
  backend: triton
  triton:
    url: localhost:8101  # gRPC endpoint
    model_name: bytetrack_tensorrt
```

**Code Example:**
```python
from core import TritonDetector

detector = TritonDetector(
    triton_url="localhost:8101",
    model_name="bytetrack_tensorrt",
    conf_thresh=0.01,
    nms_thresh=0.7,
    test_size=(640, 640),
    timeout=10.0
)

detections = detector.detect(frame)  # Returns (N, 7) array
```

---

## üéØ Decision Tree

```
Start
  ‚îÇ
  ‚îú‚îÄ Single camera?
  ‚îÇ   ‚îú‚îÄ Yes ‚Üí FPS < 20?
  ‚îÇ   ‚îÇ   ‚îú‚îÄ Yes ‚Üí PyTorch ‚úÖ
  ‚îÇ   ‚îÇ   ‚îî‚îÄ No ‚Üí TensorRT ‚úÖ
  ‚îÇ   ‚îÇ
  ‚îÇ   ‚îî‚îÄ No ‚Üí Multiple cameras?
  ‚îÇ       ‚îú‚îÄ 2-3 cameras ‚Üí TensorRT ‚úÖ
  ‚îÇ       ‚îî‚îÄ 4+ cameras ‚Üí Triton ‚úÖ
  ‚îÇ
  ‚îú‚îÄ Need low latency (< 30ms)?
  ‚îÇ   ‚îî‚îÄ Yes ‚Üí Triton ‚úÖ
  ‚îÇ
  ‚îú‚îÄ Need monitoring/metrics?
  ‚îÇ   ‚îî‚îÄ Yes ‚Üí Triton ‚úÖ
  ‚îÇ
  ‚îî‚îÄ Development/debugging?
      ‚îî‚îÄ Yes ‚Üí PyTorch ‚úÖ
```

---

## üìä Benchmark Results

### Test Setup
- **GPU**: Tesla V100-SXM2-16GB
- **Input**: 640x640 RGB images
- **Model**: ByteTrack YOLOX-X (FP16)
- **Batch Size**: 1 (single frame)
- **Iterations**: 100 warmup + 1000 test

### Single Stream Results

| Backend | Avg Time | Min Time | Max Time | Std Dev | FPS |
|---------|----------|----------|----------|---------|-----|
| PyTorch FP16 | 45.63ms | 42.1ms | 52.3ms | 2.1ms | 21.91 |
| TensorRT FP16 | 35.58ms | 33.2ms | 39.4ms | 1.5ms | 28.11 |
| Triton + TRT | **22.89ms** | **21.1ms** | **26.7ms** | **1.2ms** | **43.69** |

### Multi-Stream Results (4 cameras concurrent)

| Backend | Total Throughput | Avg Latency per Request |
|---------|------------------|------------------------|
| PyTorch (sequential) | 21.91 FPS | 182ms |
| TensorRT (sequential) | 28.11 FPS | 142ms |
| **Triton (batched)** | **66+ FPS** | **~60ms** |

**Speedup**: Triton is **3x faster** than PyTorch for multi-camera scenarios.

---

## üîß Configuration Best Practices

### PyTorch Configuration
```yaml
detection:
  backend: pytorch
  model_path: models/bytetrack_x_mot17.pth.tar
  model_type: mot17
  conf_threshold: 0.01
  nms_threshold: 0.7
  test_size: [640, 640]
  fp16: true  # Always enable for 2x speedup
  fuse: true  # Fuse Conv+BN layers
```

### TensorRT Configuration
```yaml
detection:
  backend: tensorrt
  tensorrt:
    engine_path: models/bytetrack_x_mot17_fp16.trt
    fp16: true
  conf_threshold: 0.01
  nms_threshold: 0.7
  test_size: [640, 640]
```

### Triton Configuration
```yaml
detection:
  backend: triton
  triton:
    url: localhost:8101  # gRPC endpoint
    model_name: bytetrack_tensorrt
    model_version: ''  # Empty = latest
    timeout: 10.0  # Request timeout (seconds)
    verbose: false
    
    # Dynamic batching (configured in model config.pbtxt)
    max_batch_size: 8
    max_queue_delay_ms: 0.5  # 500Œºs for low latency
    preferred_batch_sizes: [1, 2, 4, 8]
```

---

## üöÄ Migration Guide

### From PyTorch ‚Üí TensorRT

1. Export ONNX model
2. Convert to TensorRT engine
3. Update config.yaml
4. Test with single video

**Estimated time**: 15-30 minutes

### From TensorRT ‚Üí Triton

1. Setup Triton model repository
2. Copy TensorRT engine to repository
3. Create config.pbtxt
4. Start Triton Docker container
5. Update config.yaml
6. Test with multiple streams

**Estimated time**: 1-2 hours

### From PyTorch ‚Üí Triton

Combine both migrations above.

**Estimated time**: 2-3 hours

---

## üìù Summary

| Scenario | Recommended Backend | Reason |
|----------|-------------------|--------|
| Development | PyTorch | Easy debugging |
| Single camera (< 20 FPS) | PyTorch | Simple setup |
| Single camera (20-30 FPS) | TensorRT | Best single-stream performance |
| 2-3 cameras | TensorRT | Good balance |
| **4+ cameras** | **Triton** | **Dynamic batching advantage** |
| Low latency required | Triton | CUDA Graphs optimization |
| Production deployment | Triton | Monitoring, versioning, scalability |

**General Rule**: 
- Use **PyTorch** for development
- Use **TensorRT** for single-stream production
- Use **Triton** for multi-stream production

---

## üîó Related Documents

- [Triton Deployment Guide](../deployment/TRITON_DEPLOYMENT.md)
- [Stream Processing Strategy](STREAM_STRATEGY.md)
- [Configuration Guide](CONFIGURATION.md)
- [Troubleshooting](TROUBLESHOOTING.md)

---

**Last Updated**: 2025-11-11

