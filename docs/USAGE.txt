================================================================================
PERSON RE-IDENTIFICATION SYSTEM - QUICK USAGE GUIDE
================================================================================

1. EXTRACT OBJECTS FROM VIDEO (Optional - for multi-person videos)

   python scripts/extract_objects.py \
     --video data/videos/multi_person.mp4 \
     --output-dir ./output_objects \
     --model mot17 \
     --min-frames 10

   This creates separate videos for each tracked person:
   output_objects/<video_name>/object_1.mp4
   output_objects/<video_name>/object_2.mp4
   ...


2. REGISTER PERSON (First Time)

   # Register first person
   python scripts/register_mot17.py \
     --video data/videos/person.mp4 \
     --name "PersonName" \
     --global-id 1 \
     --sample-rate 5

   # Register additional person (add to existing collection)
   python scripts/register_mot17.py \
     --video data/videos/person2.mp4 \
     --name "Person2" \
     --global-id 2 \
     --sample-rate 5

   # Delete existing collection and start fresh
   python scripts/register_mot17.py \
     --video data/videos/person.mp4 \
     --name "PersonName" \
     --global-id 1 \
     --delete-existing

   ‚ö†Ô∏è  IMPORTANT:
   - Always use register_mot17.py (NOT register_person.py)
   - Each person must have a unique global-id (1, 2, 3, ...)
   - Use --delete-existing to recreate collection from scratch


3. RUN DETECTION

   python scripts/detect_and_track.py \
     --video data/videos/test.mp4 \
     --model mot17 \
     --threshold 0.8

   ‚ÑπÔ∏è  NOTE:
   - Person names are automatically retrieved from Qdrant database
   - All registered persons will be detected and labeled with their names
   - No need to specify --known-person anymore

   üöÄ REID STRATEGY (Optimized for Speed + Accuracy):
   - First 3 frames: Majority voting from 3 embeddings
   - Frame 30, 60, 90...: Re-verification (self-correction)
   - Other frames: Use cached labels (very fast)
   - Performance: ~19 FPS (5.3x faster than ReID every frame)


4. OUTPUT LOCATIONS

   outputs/videos/    - Annotated video with bounding boxes + FPS counter
   outputs/csv/       - Tracking data (frame_id, track_id, bbox, similarity, label)
   outputs/logs/      - Detailed per-frame logs (voting, re-verification events)

   üìπ VIDEO FEATURES:
   - Real-time FPS counter (top-left corner)
   - Frame counter (below FPS)
   - Person labels with similarity scores
   - Color-coded boxes: Green=Known, Red=Unknown

   üìä LOG EVENTS:
   - [VOTING]: First-3 frames majority voting (e.g., "3/3 votes ‚Üí Duong")
   - [RE-VERIFY]: Re-verification results (e.g., "Duong ‚Üí Duong (sim=0.97)")


5. PARAMETERS

   --threshold 0.8    Strict (high precision)
   --threshold 0.7    Balanced (recommended)
   --threshold 0.6    Loose (high recall)

   --max-frames 100   Process only first 100 frames (for testing)


6. EXAMPLE WORKFLOW

   # Extract objects from multi-person video
   python scripts/extract_objects.py --video data/videos/multi_cam.mp4 --output-dir ./extracted

   # Register Khiem from extracted object
   python scripts/register_mot17.py --video ./extracted/multi_cam/object_1.mp4 --name Khiem --global-id 1

   # Detect in cam2
   python scripts/detect_and_track.py --video data/videos/cam2.mkv --model mot17 --known-person Khiem --threshold 0.8

   # Detect in cam3
   python scripts/detect_and_track.py --video data/videos/cam3.mkv --model mot17 --known-person Khiem --threshold 0.8


7. TROUBLESHOOTING

   Low similarity (< 0.8)?
   ‚Üí Re-register using register_mot17.py

   Qdrant connection failed?
   ‚Üí Check configs/.env has correct credentials


================================================================================

