# ‚úÖ Triton Inference Server - DEPLOYMENT SUCCESSFUL

**Status**: ‚úÖ DEPLOYED AND RUNNING
**Date**: 2025-11-11
**Triton Version**: 24.01-py3
**GPU**: Tesla V100-SXM2-16GB

---

# Triton Inference Server Deployment Guide

## üìã OVERVIEW

H∆∞·ªõng d·∫´n deploy Person ReID System v·ªõi Triton Inference Server ƒë·ªÉ t·ªëi ∆∞u h√≥a inference cho multi-camera streaming.

### L·ª£i √≠ch c·ªßa Triton:
- **Dynamic Batching**: T·ª± ƒë·ªông g·ªôp requests t·ª´ nhi·ªÅu camera th√†nh batch
- **Concurrent Execution**: X·ª≠ l√Ω ƒë·ªìng th·ªùi nhi·ªÅu streams
- **Model Management**: Hot-reload models kh√¥ng c·∫ßn restart
- **Performance**: TƒÉng throughput 2-5x so v·ªõi single inference

---

## üìä BENCHMARK RESULTS

### Triton vs TensorRT vs PyTorch

| Backend | Avg Time (ms) | FPS | Speedup |
|---------|--------------|-----|---------|
| **PyTorch FP16** | 45.63 | 21.91 | 1.0x (baseline) |
| **TensorRT FP16** | 35.58 | 28.11 | 1.28x |
| **Triton + TensorRT** | **22.89** | **43.69** | **1.99x** |

**Key Findings**:
- ‚úÖ Triton + TensorRT: **1.99x faster** than PyTorch (22.89ms vs 45.63ms)
- ‚úÖ Triton: **1.55x faster** than TensorRT alone (22.89ms vs 35.58ms)
- ‚úÖ CUDA Graphs optimization trong Triton gi√∫p gi·∫£m latency ƒë√°ng k·ªÉ
- ‚úÖ Throughput tƒÉng t·ª´ 21.91 FPS ‚Üí 43.69 FPS (g·∫ßn g·∫•p ƒë√¥i)

**Note**: Benchmark v·ªõi batch_size=1. Multi-stream concurrent inference s·∫Ω c√≥ throughput cao h∆°n n·ªØa.

---

## üèóÔ∏è ARCHITECTURE

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Camera 1   ‚îÇ  ‚îÇ  Camera 2   ‚îÇ  ‚îÇ  Camera N   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                ‚îÇ                ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ  Detection     ‚îÇ
                ‚îÇ  Service       ‚îÇ
                ‚îÇ  (FastAPI)     ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ gRPC
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ  Triton Server ‚îÇ
                ‚îÇ  (Dynamic      ‚îÇ
                ‚îÇ   Batching)    ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ  TensorRT      ‚îÇ
                ‚îÇ  Engine        ‚îÇ
                ‚îÇ  (FP16)        ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üöÄ DEPLOYMENT STEPS

### Step 1: Prepare TensorRT Engine

```bash
# Activate environment
source ~/data/hai_venv_py310/bin/activate

# Convert ONNX to TensorRT (if not done)
python tools/convert_tensorrt.py \
    --onnx models/bytetrack_x_mot17_fp32.onnx \
    --output models/bytetrack_x_mot17_fp16.trt \
    --fp16 \
    --workspace 4096

# Verify engine
ls -lh models/bytetrack_x_mot17_fp16.trt
```

### Step 2: Setup Triton Model Repository

```bash
cd deployment

# Run setup script
bash setup_triton.sh
```

**Expected output:**
```
‚úÖ Found TensorRT engine: ../models/bytetrack_x_mot17_fp16.trt (192M)
‚úÖ Model repository ready at: ../triton_model_repository
```

**Directory structure:**
```
triton_model_repository/
‚îî‚îÄ‚îÄ bytetrack_tensorrt/
    ‚îú‚îÄ‚îÄ config.pbtxt          # Model configuration
    ‚îî‚îÄ‚îÄ 1/
        ‚îî‚îÄ‚îÄ model.plan        # TensorRT engine (192MB)
```

### Step 3: Start Triton Server

```bash
# Start only Triton service
cd deployment
sudo docker compose up -d triton

# Check logs
sudo docker compose logs -f triton

# Wait for "Started GRPCInferenceService at 0.0.0.0:8101"
# Wait for "Started HTTPService at 0.0.0.0:8100"
```

**Important**: Triton uses custom ports to avoid conflicts:
- HTTP: `8100` (instead of default 8000)
- gRPC: `8101` (instead of default 8001)
- Metrics: `8102` (instead of default 8002)

### Step 4: Verify Triton Server

```bash
# Check health
curl http://localhost:8100/v2/health/ready

# Expected: HTTP 200 OK (empty response)

# Check model status
curl http://localhost:8100/v2/models/bytetrack_tensorrt

# Expected output:
# {
#   "name":"bytetrack_tensorrt",
#   "versions":["1"],
#   "platform":"tensorrt_plan",
#   "inputs":[{"name":"images","datatype":"FP32","shape":[-1,3,640,640]}],
#   "outputs":[{"name":"output","datatype":"FP32","shape":[-1,8400,6]}]
# }
```

### Step 5: Update Configuration

Edit `configs/config.yaml`:

```yaml
detection:
  backend: triton  # Change from pytorch/tensorrt to triton

  triton:
    url: localhost:8101  # gRPC endpoint (custom port)
  
  triton:
    url: localhost:8001
    model_name: bytetrack_tensorrt
    model_version: ''
    timeout: 10.0
    verbose: false
```

### Step 6: Start Detection Service

```bash
# Build and start detection service
docker-compose up -d --build detection

# Check logs
docker-compose logs -f detection

# Wait for "Application startup complete"
```

### Step 7: Start All Services

```bash
# Start all services
docker-compose up -d

# Check status
docker-compose ps
```

**Expected output:**
```
NAME                        STATUS              PORTS
triton_inference_server     Up (healthy)        8000-8002
person_reid_detection       Up (healthy)        8003
person_reid_extract         Up (healthy)        8001
person_reid_register        Up (healthy)        8002
person_reid_ui              Up (healthy)        8501
```

---

## üß™ TESTING

### Test 1: Triton Server Health

```bash
# Server health
curl http://localhost:8000/v2/health/live
curl http://localhost:8000/v2/health/ready

# Model metadata
curl http://localhost:8000/v2/models/bytetrack_tensorrt/config

# Model statistics
curl http://localhost:8000/v2/models/bytetrack_tensorrt/stats
```

### Test 2: Single Camera Inference

```bash
# Test detection API
curl -X POST http://localhost:8003/detect \
  -F "file=@test_image.jpg"
```

### Test 3: Multi-Camera Concurrent Inference

```python
import requests
import concurrent.futures
import time

def detect_frame(camera_id, frame_path):
    url = "http://localhost:8003/detect"
    files = {"file": open(frame_path, "rb")}
    start = time.time()
    response = requests.post(url, files=files)
    latency = time.time() - start
    return camera_id, latency, response.status_code

# Simulate 4 cameras
cameras = [
    (1, "camera1_frame.jpg"),
    (2, "camera2_frame.jpg"),
    (3, "camera3_frame.jpg"),
    (4, "camera4_frame.jpg"),
]

# Concurrent requests
with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
    futures = [executor.submit(detect_frame, cam_id, path) for cam_id, path in cameras]
    results = [f.result() for f in futures]

for cam_id, latency, status in results:
    print(f"Camera {cam_id}: {latency*1000:.1f}ms (status={status})")
```

---

## üìä PERFORMANCE TUNING

### Dynamic Batching Configuration

Edit `triton_model_repository/bytetrack_tensorrt/config.pbtxt`:

```protobuf
dynamic_batching {
  # Preferred batch sizes
  preferred_batch_size: [ 1, 2, 4, 8 ]
  
  # Max queue delay (microseconds)
  # Lower = lower latency, higher = better throughput
  max_queue_delay_microseconds: 5000  # 5ms
  
  # Preserve ordering
  preserve_ordering: false  # Set true if order matters
}
```

**Tuning guidelines:**

| Use Case | max_queue_delay_microseconds | preferred_batch_size |
|----------|------------------------------|----------------------|
| Low latency (1-2 cameras) | 1000-2000 (1-2ms) | [1, 2] |
| Balanced (3-4 cameras) | 5000 (5ms) | [1, 2, 4] |
| High throughput (8+ cameras) | 10000-20000 (10-20ms) | [2, 4, 8, 16] |

### Instance Configuration

For multiple GPUs:

```protobuf
instance_group [
  {
    count: 1
    kind: KIND_GPU
    gpus: [ 0 ]  # GPU 0
  },
  {
    count: 1
    kind: KIND_GPU
    gpus: [ 1 ]  # GPU 1
  }
]
```

### CUDA Graphs Optimization

Enable CUDA graphs for faster inference:

```protobuf
optimization {
  cuda {
    graphs: true
    graph_spec {
      batch_size: 1
      input { ... }
    }
    graph_spec {
      batch_size: 4
      input { ... }
    }
  }
}
```

---

## üìà MONITORING

### Triton Metrics

```bash
# Prometheus metrics endpoint
curl http://localhost:8002/metrics

# Key metrics:
# - nv_inference_request_success
# - nv_inference_request_duration_us
# - nv_inference_queue_duration_us
# - nv_gpu_utilization
```

### Docker Stats

```bash
# Real-time resource usage
docker stats triton_inference_server

# Expected:
# - GPU Memory: 2-4GB
# - CPU: 10-30%
# - Memory: 4-8GB
```

### Logs

```bash
# Triton logs
docker-compose logs -f triton | grep -E "(Loading|Ready|Error)"

# Detection service logs
docker-compose logs -f detection | grep -E "(Triton|inference|batch)"
```

---

## üêõ TROUBLESHOOTING

### Issue 1: Triton server not starting

**Symptoms:**
```
Error: failed to load model 'bytetrack_tensorrt'
```

**Solution:**
```bash
# Check model repository structure
ls -R triton_model_repository/

# Verify TensorRT engine
file triton_model_repository/bytetrack_tensorrt/1/model.plan

# Check config
cat triton_model_repository/bytetrack_tensorrt/config.pbtxt
```

### Issue 2: Model not ready

**Symptoms:**
```
curl http://localhost:8000/v2/models/bytetrack_tensorrt
# Returns: "state": "UNAVAILABLE"
```

**Solution:**
```bash
# Check Triton logs
docker-compose logs triton | tail -50

# Common issues:
# - Wrong input/output names in config.pbtxt
# - TensorRT engine incompatible with GPU
# - Insufficient GPU memory
```

### Issue 3: High latency with dynamic batching

**Symptoms:**
- Single request latency > 100ms
- Batch size always 1

**Solution:**
```bash
# Reduce max_queue_delay_microseconds
# Edit config.pbtxt:
max_queue_delay_microseconds: 1000  # 1ms instead of 5ms

# Reload model
docker-compose restart triton
```

### Issue 4: Out of GPU memory

**Symptoms:**
```
Error: CUDA out of memory
```

**Solution:**
```bash
# Reduce max_batch_size in config.pbtxt
max_batch_size: 4  # Instead of 8

# Or reduce instance count
instance_group [
  {
    count: 1  # Instead of 2
    kind: KIND_GPU
    gpus: [ 0 ]
  }
]
```

---

## üîÑ UPDATES & MAINTENANCE

### Update TensorRT Engine

```bash
# 1. Convert new engine
python tools/convert_tensorrt.py --onnx new_model.onnx --output new_engine.trt --fp16

# 2. Copy to model repository
cp new_engine.trt triton_model_repository/bytetrack_tensorrt/2/model.plan

# 3. Triton will auto-reload (if model_control_mode=poll)
# Or manually reload:
curl -X POST http://localhost:8000/v2/repository/models/bytetrack_tensorrt/load
```

### Scale Instances

```bash
# Horizontal scaling (more containers)
docker-compose up -d --scale detection=3

# Vertical scaling (more GPU instances)
# Edit config.pbtxt and increase instance count
```

### Backup & Restore

```bash
# Backup model repository
tar -czf triton_models_backup.tar.gz triton_model_repository/

# Restore
tar -xzf triton_models_backup.tar.gz
docker-compose restart triton
```

---

## üìä EXPECTED PERFORMANCE

### Single Camera (Baseline)

| Backend | Latency | Throughput |
|---------|---------|------------|
| PyTorch | 45ms | 22 FPS |
| TensorRT | 35ms | 28 FPS |
| Triton (batch=1) | 38ms | 26 FPS |

### Multi-Camera (4 cameras concurrent)

| Backend | Avg Latency | Total Throughput |
|---------|-------------|------------------|
| PyTorch (sequential) | 180ms | 22 FPS |
| TensorRT (sequential) | 140ms | 28 FPS |
| **Triton (batch=4)** | **60ms** | **66 FPS** |

**Speedup: 2.4x vs TensorRT, 3x vs PyTorch**

### Multi-Camera (8 cameras concurrent)

| Backend | Avg Latency | Total Throughput |
|---------|-------------|------------------|
| Triton (batch=8) | 90ms | 88 FPS |

**Speedup: 3.1x vs TensorRT**

---

## üìù NOTES

1. **Dynamic batching trade-off:**
   - Lower `max_queue_delay` = lower latency, lower throughput
   - Higher `max_queue_delay` = higher latency, higher throughput

2. **GPU memory:**
   - Batch size 1: ~2GB
   - Batch size 4: ~3GB
   - Batch size 8: ~4GB

3. **Network mode:**
   - Using `host` mode for lowest latency
   - For production, consider bridge mode with proper port mapping

4. **Security:**
   - Triton has no authentication by default
   - Use reverse proxy (nginx) with auth for production
   - Restrict network access with firewall rules

---

## üîó REFERENCES

- [Triton Inference Server Docs](https://docs.nvidia.com/deeplearning/triton-inference-server/)
- [TensorRT Backend](https://github.com/triton-inference-server/tensorrt_backend)
- [Dynamic Batching](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#dynamic-batcher)

---

**Last Updated:** 2025-11-11  
**Triton Version:** 24.01  
**TensorRT Version:** 8.6.1

