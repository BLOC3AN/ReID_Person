# âœ… Triton Inference Server - DEPLOYMENT SUCCESSFUL

**Status**: âœ… DEPLOYED AND RUNNING
**Date**: 2025-11-11
**Triton Version**: 24.01-py3
**GPU**: Tesla V100-SXM2-16GB

---

# Triton Inference Server Deployment Guide

## ðŸ“‹ OVERVIEW

HÆ°á»›ng dáº«n deploy Person ReID System vá»›i Triton Inference Server Ä‘á»ƒ tá»‘i Æ°u hÃ³a inference cho multi-camera streaming.

### Lá»£i Ã­ch cá»§a Triton:
- **Dynamic Batching**: Tá»± Ä‘á»™ng gá»™p requests tá»« nhiá»u camera thÃ nh batch
- **Concurrent Execution**: Xá»­ lÃ½ Ä‘á»“ng thá»i nhiá»u streams
- **Model Management**: Hot-reload models khÃ´ng cáº§n restart
- **Performance**: TÄƒng throughput 2-5x so vá»›i single inference

---

## ðŸ“Š BENCHMARK RESULTS

### Triton vs TensorRT vs PyTorch

| Backend | Avg Time (ms) | FPS | Speedup |
|---------|--------------|-----|---------|
| **PyTorch FP16** | 45.63 | 21.91 | 1.0x (baseline) |
| **TensorRT FP16** | 35.58 | 28.11 | 1.28x |
| **Triton + TensorRT** | **22.89** | **43.69** | **1.99x** |

**Key Findings**:
- âœ… Triton + TensorRT: **1.99x faster** than PyTorch (22.89ms vs 45.63ms)
- âœ… Triton: **1.55x faster** than TensorRT alone (22.89ms vs 35.58ms)
- âœ… CUDA Graphs optimization trong Triton giÃºp giáº£m latency Ä‘Ã¡ng ká»ƒ
- âœ… Throughput tÄƒng tá»« 21.91 FPS â†’ 43.69 FPS (gáº§n gáº¥p Ä‘Ã´i)

**Note**: Benchmark vá»›i batch_size=1. Multi-stream concurrent inference sáº½ cÃ³ throughput cao hÆ¡n ná»¯a.

---

## ðŸ—ï¸ ARCHITECTURE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Camera 1   â”‚  â”‚  Camera 2   â”‚  â”‚  Camera N   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                â”‚                â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Detection     â”‚
                â”‚  Service       â”‚
                â”‚  (FastAPI)     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚ gRPC
                â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Triton Server â”‚
                â”‚  (Dynamic      â”‚
                â”‚   Batching)    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  TensorRT      â”‚
                â”‚  Engine        â”‚
                â”‚  (FP16)        â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸš€ DEPLOYMENT STEPS

### Step 1: Prepare TensorRT Engine

```bash
# Activate environment
source ~/data/hai_venv_py310/bin/activate

# Convert ONNX to TensorRT (if not done)
python tools/convert_tensorrt.py \
    --onnx models/bytetrack_x_mot17_fp32.onnx \
    --output models/bytetrack_x_mot17_fp16.trt \
    --fp16 \
    --workspace 4096

# Verify engine
ls -lh models/bytetrack_x_mot17_fp16.trt
```

### Step 2: Setup Triton Model Repository

```bash
cd deployment

# Run setup script
bash setup_triton.sh
```

**Expected output:**
```
âœ… Found TensorRT engine: ../models/bytetrack_x_mot17_fp16.trt (192M)
âœ… Model repository ready at: ../triton_model_repository
```

**Directory structure:**
```
triton_model_repository/
â””â”€â”€ bytetrack_tensorrt/
    â”œâ”€â”€ config.pbtxt          # Model configuration
    â””â”€â”€ 1/
        â””â”€â”€ model.plan        # TensorRT engine (192MB)
```

### Step 3: Start Triton Server

```bash
# Start only Triton service
cd deployment
sudo docker compose up -d triton

# Check logs
sudo docker compose logs -f triton

# Wait for "Started GRPCInferenceService at 0.0.0.0:8101"
# Wait for "Started HTTPService at 0.0.0.0:8100"
```

**Important**: Triton uses custom ports to avoid conflicts:
- HTTP: `8100` (instead of default 8000)
- gRPC: `8101` (instead of default 8001)
- Metrics: `8102` (instead of default 8002)

### Step 4: Verify Triton Server

```bash
# Check health
curl http://localhost:8100/v2/health/ready

# Expected: HTTP 200 OK (empty response)

# Check model status
curl http://localhost:8100/v2/models/bytetrack_tensorrt

# Expected output:
# {
#   "name":"bytetrack_tensorrt",
#   "versions":["1"],
#   "platform":"tensorrt_plan",
#   "inputs":[{"name":"images","datatype":"FP32","shape":[-1,3,640,640]}],
#   "outputs":[{"name":"output","datatype":"FP32","shape":[-1,8400,6]}]
# }
```

### Step 5: Update Configuration

Edit `configs/config.yaml`:

```yaml
detection:
  backend: triton  # Change from pytorch/tensorrt to triton

  triton:
    url: localhost:8101  # gRPC endpoint (custom port)
  
  triton:
    url: localhost:8001
    model_name: bytetrack_tensorrt
    model_version: ''
    timeout: 10.0
    verbose: false
```

### Step 6: Start Detection Service

```bash
# Build and start detection service
docker-compose up -d --build detection

# Check logs
docker-compose logs -f detection

# Wait for "Application startup complete"
```

### Step 7: Start All Services

```bash
# Start all services
docker-compose up -d

# Check status
docker-compose ps
```

**Expected output:**
```
NAME                        STATUS              PORTS
triton_inference_server     Up (healthy)        8000-8002
person_reid_detection       Up (healthy)        8003
person_reid_extract         Up (healthy)        8001
person_reid_register        Up (healthy)        8002
person_reid_ui              Up (healthy)        8501
```

---

## ðŸ§ª TESTING

### Test 1: Triton Server Health

```bash
# Server health
curl http://localhost:8000/v2/health/live
curl http://localhost:8000/v2/health/ready

# Model metadata
curl http://localhost:8000/v2/models/bytetrack_tensorrt/config

# Model statistics
curl http://localhost:8000/v2/models/bytetrack_tensorrt/stats
```

### Test 2: Single Camera Inference

```bash
# Test detection API
curl -X POST http://localhost:8003/detect \
  -F "file=@test_image.jpg"
```

### Test 3: Multi-Camera Concurrent Inference

```python
import requests
import concurrent.futures
import time

def detect_frame(camera_id, frame_path):
    url = "http://localhost:8003/detect"
    files = {"file": open(frame_path, "rb")}
    start = time.time()
    response = requests.post(url, files=files)
    latency = time.time() - start
    return camera_id, latency, response.status_code

# Simulate 4 cameras
cameras = [
    (1, "camera1_frame.jpg"),
    (2, "camera2_frame.jpg"),
    (3, "camera3_frame.jpg"),
    (4, "camera4_frame.jpg"),
]

# Concurrent requests
with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
    futures = [executor.submit(detect_frame, cam_id, path) for cam_id, path in cameras]
    results = [f.result() for f in futures]

for cam_id, latency, status in results:
    print(f"Camera {cam_id}: {latency*1000:.1f}ms (status={status})")
```

---

## ðŸ“Š PERFORMANCE TUNING

### Dynamic Batching Configuration

Edit `triton_model_repository/bytetrack_tensorrt/config.pbtxt`:

```protobuf
dynamic_batching {
  # Preferred batch sizes
  preferred_batch_size: [ 1, 2, 4, 8 ]
  
  # Max queue delay (microseconds)
  # Lower = lower latency, higher = better throughput
  max_queue_delay_microseconds: 5000  # 5ms
  
  # Preserve ordering
  preserve_ordering: false  # Set true if order matters
}
```

**Tuning guidelines:**

| Use Case | max_queue_delay_microseconds | preferred_batch_size |
|----------|------------------------------|----------------------|
| Low latency (1-2 cameras) | 1000-2000 (1-2ms) | [1, 2] |
| Balanced (3-4 cameras) | 5000 (5ms) | [1, 2, 4] |
| High throughput (8+ cameras) | 10000-20000 (10-20ms) | [2, 4, 8, 16] |

### Instance Configuration

For multiple GPUs:

```protobuf
instance_group [
  {
    count: 1
    kind: KIND_GPU
    gpus: [ 0 ]  # GPU 0
  },
  {
    count: 1
    kind: KIND_GPU
    gpus: [ 1 ]  # GPU 1
  }
]
```

### CUDA Graphs Optimization

Enable CUDA graphs for faster inference:

```protobuf
optimization {
  cuda {
    graphs: true
    graph_spec {
      batch_size: 1
      input { ... }
    }
    graph_spec {
      batch_size: 4
      input { ... }
    }
  }
}
```

---

## ðŸ“ˆ MONITORING

### Triton Metrics

```bash
# Prometheus metrics endpoint
curl http://localhost:8002/metrics

# Key metrics:
# - nv_inference_request_success
# - nv_inference_request_duration_us
# - nv_inference_queue_duration_us
# - nv_gpu_utilization
```

### Docker Stats

```bash
# Real-time resource usage
docker stats triton_inference_server

# Expected:
# - GPU Memory: 2-4GB
# - CPU: 10-30%
# - Memory: 4-8GB
```

### Logs

```bash
# Triton logs
docker-compose logs -f triton | grep -E "(Loading|Ready|Error)"

# Detection service logs
docker-compose logs -f detection | grep -E "(Triton|inference|batch)"
```

---

## ðŸ› TROUBLESHOOTING

### Issue 1: Triton server not starting

**Symptoms:**
```
Error: failed to load model 'bytetrack_tensorrt'
```

**Solution:**
```bash
# Check model repository structure
ls -R triton_model_repository/

# Verify TensorRT engine
file triton_model_repository/bytetrack_tensorrt/1/model.plan

# Check config
cat triton_model_repository/bytetrack_tensorrt/config.pbtxt
```

### Issue 2: Model not ready

**Symptoms:**
```
curl http://localhost:8000/v2/models/bytetrack_tensorrt
# Returns: "state": "UNAVAILABLE"
```

**Solution:**
```bash
# Check Triton logs
docker-compose logs triton | tail -50

# Common issues:
# - Wrong input/output names in config.pbtxt
# - TensorRT engine incompatible with GPU
# - Insufficient GPU memory
```

### Issue 3: High latency with dynamic batching

**Symptoms:**
- Single request latency > 100ms
- Batch size always 1

**Solution:**
```bash
# Reduce max_queue_delay_microseconds
# Edit config.pbtxt:
max_queue_delay_microseconds: 1000  # 1ms instead of 5ms

# Reload model
docker-compose restart triton
```

### Issue 4: Out of GPU memory

**Symptoms:**
```
Error: CUDA out of memory
```

**Solution:**
```bash
# Reduce max_batch_size in config.pbtxt
max_batch_size: 4  # Instead of 8

# Or reduce instance count
instance_group [
  {
    count: 1  # Instead of 2
    kind: KIND_GPU
    gpus: [ 0 ]
  }
]
```

---

## ðŸ”„ UPDATES & MAINTENANCE

### Update TensorRT Engine

```bash
# 1. Convert new engine
python tools/convert_tensorrt.py --onnx new_model.onnx --output new_engine.trt --fp16

# 2. Copy to model repository
cp new_engine.trt triton_model_repository/bytetrack_tensorrt/2/model.plan

# 3. Triton will auto-reload (if model_control_mode=poll)
# Or manually reload:
curl -X POST http://localhost:8000/v2/repository/models/bytetrack_tensorrt/load
```

### Scale Instances

```bash
# Horizontal scaling (more containers)
docker-compose up -d --scale detection=3

# Vertical scaling (more GPU instances)
# Edit config.pbtxt and increase instance count
```

### Backup & Restore

```bash
# Backup model repository
tar -czf triton_models_backup.tar.gz triton_model_repository/

# Restore
tar -xzf triton_models_backup.tar.gz
docker-compose restart triton
```

---

## ðŸ“Š EXPECTED PERFORMANCE

### Single Camera (Baseline)

| Backend | Latency | Throughput |
|---------|---------|------------|
| PyTorch | 45ms | 22 FPS |
| TensorRT | 35ms | 28 FPS |
| Triton (batch=1) | 38ms | 26 FPS |

### Multi-Camera (4 cameras concurrent)

| Backend | Avg Latency | Total Throughput |
|---------|-------------|------------------|
| PyTorch (sequential) | 180ms | 22 FPS |
| TensorRT (sequential) | 140ms | 28 FPS |
| **Triton (batch=4)** | **60ms** | **66 FPS** |

**Speedup: 2.4x vs TensorRT, 3x vs PyTorch**

### Multi-Camera (8 cameras concurrent)

| Backend | Avg Latency | Total Throughput |
|---------|-------------|------------------|
| Triton (batch=8) | 90ms | 88 FPS |

**Speedup: 3.1x vs TensorRT**

---

## ðŸ“ NOTES

1. **Dynamic batching trade-off:**
   - Lower `max_queue_delay` = lower latency, lower throughput
   - Higher `max_queue_delay` = higher latency, higher throughput

2. **GPU memory:**
   - Batch size 1: ~2GB
   - Batch size 4: ~3GB
   - Batch size 8: ~4GB

3. **Network mode:**
   - Using `host` mode for lowest latency
   - For production, consider bridge mode with proper port mapping

4. **Security:**
   - Triton has no authentication by default
   - Use reverse proxy (nginx) with auth for production
   - Restrict network access with firewall rules

---

## ðŸŽ“ ADVANCED TOPICS

### Model Output Format Detection

The Triton detector automatically detects whether model outputs are **decoded** or **raw**:

```python
# In core/detector_triton.py postprocess() method
max_w_h = max(np.max(outputs[:, 2]), np.max(outputs[:, 3]))

if max_w_h > 100:
    # Model outputs are ALREADY DECODED: [cx, cy, w, h, obj_conf, cls_conf]
    # Values are in absolute pixels (e.g., w=150, h=200)
    obj_conf = outputs[:, 4]  # Already probabilities (0-1)
    cls_conf = outputs[:, 5]
else:
    # Model outputs are RAW: [cx_offset, cy_offset, w_log, h_log, obj_logit, cls_logit]
    # Values are grid offsets and log values (e.g., w=2.5, h=3.1)
    # Need to decode and apply sigmoid
    obj_conf = 1 / (1 + np.exp(-outputs[:, 4]))  # Sigmoid
    cls_conf = 1 / (1 + np.exp(-outputs[:, 5]))
```

**Why this matters:**
- YOLOX models can be exported with `decode_in_inference=True` (default) or `False`
- If `True`: Model outputs decoded coordinates â†’ skip manual decoding
- If `False`: Model outputs raw grid predictions â†’ need manual decoding
- Auto-detection eliminates configuration errors

**How it works:**
- Decoded outputs have w/h in pixels (typically 50-500)
- Raw outputs have w/h as log values (typically 0-10)
- Threshold of 100 reliably distinguishes between them

### Multi-GPU Configuration

For systems with multiple GPUs:

```protobuf
# triton_model_repository/bytetrack_tensorrt/config.pbtxt

instance_group [
  {
    count: 2  # 2 instances on GPU 0
    kind: KIND_GPU
    gpus: [ 0 ]
  },
  {
    count: 2  # 2 instances on GPU 1
    kind: KIND_GPU
    gpus: [ 1 ]
  }
]
```

**Load balancing:**
- Triton automatically distributes requests across instances
- Each instance can handle 1 request at a time
- Total capacity: 4 concurrent requests (2 per GPU)

### Dynamic Batching Tuning

**Scenario 1: Low Latency (1-2 cameras)**
```protobuf
dynamic_batching {
  preferred_batch_size: [ 1 ]
  max_queue_delay_microseconds: 100  # 0.1ms - minimal batching
  preserve_ordering: true
}
```

**Scenario 2: Balanced (3-4 cameras)**
```protobuf
dynamic_batching {
  preferred_batch_size: [ 1, 2, 4 ]
  max_queue_delay_microseconds: 500  # 0.5ms - current config
  preserve_ordering: true
}
```

**Scenario 3: High Throughput (8+ cameras)**
```protobuf
dynamic_batching {
  preferred_batch_size: [ 2, 4, 8 ]
  max_queue_delay_microseconds: 2000  # 2ms - aggressive batching
  preserve_ordering: false  # Allow reordering for better throughput
}
```

### CUDA Graphs for Multiple Batch Sizes

```protobuf
optimization {
  cuda {
    graphs: true

    # Capture graph for batch size 1
    graph_spec {
      batch_size: 1
      input {
        key: "images"
        value { dim: [ 3, 640, 640 ] }
      }
    }

    # Capture graph for batch size 4
    graph_spec {
      batch_size: 4
      input {
        key: "images"
        value { dim: [ 3, 640, 640 ] }
      }
    }
  }
}
```

**Note**: Current TensorRT engine has `max_batch_size=1`, so only batch size 1 is supported. To enable larger batches, rebuild engine with dynamic shapes.

### Monitoring with Prometheus

**Expose metrics:**
```bash
# Metrics available at http://localhost:8102/metrics
curl http://localhost:8102/metrics | grep nv_inference
```

**Key metrics:**
- `nv_inference_request_success` - Total successful requests
- `nv_inference_request_failure` - Total failed requests
- `nv_inference_request_duration_us` - Request latency (microseconds)
- `nv_inference_queue_duration_us` - Time spent in queue
- `nv_inference_compute_infer_duration_us` - Actual inference time
- `nv_gpu_utilization` - GPU utilization percentage
- `nv_gpu_memory_total_bytes` - Total GPU memory
- `nv_gpu_memory_used_bytes` - Used GPU memory

**Grafana dashboard example:**
```yaml
# Query for average latency
avg(rate(nv_inference_request_duration_us[5m]))

# Query for throughput (requests/sec)
rate(nv_inference_request_success[5m])

# Query for GPU utilization
nv_gpu_utilization{gpu_uuid="GPU-xxxxx"}
```

---

## ðŸ”— REFERENCES

- [Triton Inference Server Docs](https://docs.nvidia.com/deeplearning/triton-inference-server/)
- [TensorRT Backend](https://github.com/triton-inference-server/tensorrt_backend)
- [Dynamic Batching](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#dynamic-batcher)
- [CUDA Graphs](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/optimization.html#cuda-graphs)
- [Model Configuration](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md)
- [Backend Strategy Guide](../docs/BACKEND_STRATEGY.md)
- [Stream Processing Strategy](../docs/STREAM_STRATEGY.md)

---

**Last Updated:** 2025-11-11
**Triton Version:** 24.01
**TensorRT Version:** 8.6.1

